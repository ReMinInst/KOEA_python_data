{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Week 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "November 9, 2019\n",
    "\n",
    "If the installation was done properly on October 26, we are ready to start machine learning projects.\n",
    "In the following weeks, we'll make a team and pick a project that you are interested in. In order to give you better ideas and understand what AI can do, we'll go over a few examples drawn from engineering and science. \n",
    "\n",
    "Today, let's start with number recognition and image denoising using autoencoder.\n",
    "We'll use the dataset that we previously played with for our plotting. So the data should be in your cloned repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jindogae/anaconda3/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using tensorflow  1.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow \n",
    "\n",
    "if tensorflow.__version__ < '2.0.0':\n",
    "    import keras \n",
    "    from keras.models import Model\n",
    "    from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Dense, Dropout\n",
    "    print(\"using tensorflow \", tensorflow.__version__)\n",
    "elif tensorflow.__version__ >= '2.0.0':\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Dense, Dropout\n",
    "    print(\"using tensorflow \", tensorflow.__version__)\n",
    "## keras functions are well documented in the Keras Documentaion \n",
    "## https://keras.io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest3D = np.load('xtest.pickle', allow_pickle=True)\n",
    "Ytest = np.load('ytest.pickle', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xtrain3D = np.load('xtrain.pickle', allow_pickle=True)\n",
    "#Ytrain = np.load('ytrain.pickle', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## original images have unsigned integer from 0 to 255 (8bit representation of gray scale color) \n",
    "## Since the convolutions are numerical computation, we change the data type to float32 and then \n",
    "## perform normalizations to [0,1] range\n",
    "\n",
    "Xtest3D = Xtest3D.astype('float32')/255.\n",
    "#Xtrain3D = Xtrain3D.astype('float32')/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## in order to conform to the shape of inputs to Conv2D\n",
    "## Conv2D expects 4D array\n",
    "## 1st index - sample index\n",
    "## 2nd index - image x\n",
    "## 3rd index - image y\n",
    "## 4th index - channel (Ex. RGB values in color images)\n",
    "\n",
    "Xtest = Xtest3D.reshape((*Xtest3D.shape, 1))\n",
    "#Xtrain = Xtrain3D.reshape((*Xtrain3D.shape, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Since the computing power and time is limitted, let's reduce the size of training and test datasets \n",
    "\n",
    "Xtrain = Xtest[0:600,:]\n",
    "Ytrain = Ytest[0:600]\n",
    "\n",
    "Xtest = Xtest[1000:1100,:]\n",
    "Ytest = Ytest[1000:1100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 28, 28, 1) (100,)\n",
      "float32 uint8\n"
     ]
    }
   ],
   "source": [
    "## Let's check the shape and type of the our test dataset\n",
    "\n",
    "print(Xtest.shape, Ytest.shape)\n",
    "print(Xtest.dtype, Ytest.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 28, 28, 1) (600,)\n",
      "float32 uint8\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain.shape, Ytrain.shape)\n",
    "print(Xtrain.dtype, Ytrain.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's define our neural network model that consists of Convolutional, Flattening, MaxPooling and UpSampling layers\n",
    "## We can go deep into each layers and optimization if necessary and if our members are interested.\n",
    "\n",
    "def model(choice = 2):\n",
    "    '''\n",
    "    input parameters\n",
    "      choice - 1, classification of mnist handwriting images\n",
    "               2, encoding and decoding for denoising\n",
    "               \n",
    "    output\n",
    "      keras model defining the network from an input to the final output\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    input_img = Input(shape=(28, 28, 1))\n",
    "    x1 = Conv2D(32, (3,3), activation='relu', padding='same')(input_img)\n",
    "    x2 = MaxPooling2D((2,2), padding='same')(x1)\n",
    "    x3 = Conv2D(32, (3,3), activation='relu', padding='same')(x2)\n",
    "    \n",
    "    encoded = MaxPooling2D((2,2), padding='same')(x3)\n",
    "    c1 = Flatten()(encoded)\n",
    "    c2 = Dense(128, activation='relu')(c1)\n",
    "    c3 = Dropout(0.2)(c2)\n",
    "    \n",
    "    ## this is the final output for classification\n",
    "    classifierOutput = Dense(10, activation='softmax')(c3)\n",
    "    \n",
    "    ## The following layers from y4 to decoded is for denoising\n",
    "    ## we'll cover this in the next meeting\n",
    "    y4 = Conv2D(32, (3,3), activation='relu', padding='same')(encoded)\n",
    "    y3 = UpSampling2D((2,2))(y4)\n",
    "    y2 = Conv2D(32, (3,3), activation='relu', padding='same')(y3)\n",
    "    y1 = UpSampling2D((2,2))(y2)\n",
    "    \n",
    "    decoded = Conv2D(1, (3,3), activation='sigmoid', padding='same')(y1)\n",
    "    \n",
    "    if choice == 1:\n",
    "        return Model(input_img, classifierOutput)\n",
    "    else: \n",
    "        \n",
    "        ## This part is for the next meeting\n",
    "        \n",
    "        return Model(input_img, decoded)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we call a function and get our model defintion.\n",
    "myclassifier = model(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Once a model is defined, we need to configure the model for trainging \n",
    "## by selecting optimizer and loss function\n",
    "\n",
    "myclassifier.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 2.2240 - acc: 0.1850\n",
      "Epoch 2/20\n",
      "600/600 [==============================] - 0s 363us/step - loss: 1.8764 - acc: 0.6167\n",
      "Epoch 3/20\n",
      "600/600 [==============================] - 0s 342us/step - loss: 1.3376 - acc: 0.6817\n",
      "Epoch 4/20\n",
      "600/600 [==============================] - 0s 358us/step - loss: 0.9138 - acc: 0.7183\n",
      "Epoch 5/20\n",
      "600/600 [==============================] - 0s 347us/step - loss: 0.7167 - acc: 0.7583\n",
      "Epoch 6/20\n",
      "600/600 [==============================] - 0s 368us/step - loss: 0.5761 - acc: 0.8133\n",
      "Epoch 7/20\n",
      "600/600 [==============================] - 0s 348us/step - loss: 0.4569 - acc: 0.8483\n",
      "Epoch 8/20\n",
      "600/600 [==============================] - 0s 360us/step - loss: 0.3635 - acc: 0.8817\n",
      "Epoch 9/20\n",
      "600/600 [==============================] - 0s 355us/step - loss: 0.3166 - acc: 0.9000\n",
      "Epoch 10/20\n",
      "600/600 [==============================] - 0s 341us/step - loss: 0.2825 - acc: 0.9050\n",
      "Epoch 11/20\n",
      "600/600 [==============================] - 0s 356us/step - loss: 0.2757 - acc: 0.9083\n",
      "Epoch 12/20\n",
      "600/600 [==============================] - 0s 364us/step - loss: 0.2099 - acc: 0.9283\n",
      "Epoch 13/20\n",
      "600/600 [==============================] - 0s 364us/step - loss: 0.1781 - acc: 0.9417\n",
      "Epoch 14/20\n",
      "600/600 [==============================] - 0s 353us/step - loss: 0.1627 - acc: 0.9317\n",
      "Epoch 15/20\n",
      "600/600 [==============================] - 0s 364us/step - loss: 0.1520 - acc: 0.9517\n",
      "Epoch 16/20\n",
      "600/600 [==============================] - 0s 347us/step - loss: 0.1374 - acc: 0.9517\n",
      "Epoch 17/20\n",
      "600/600 [==============================] - 0s 364us/step - loss: 0.1184 - acc: 0.9650\n",
      "Epoch 18/20\n",
      "600/600 [==============================] - 0s 372us/step - loss: 0.0889 - acc: 0.9717\n",
      "Epoch 19/20\n",
      "600/600 [==============================] - 0s 354us/step - loss: 0.0826 - acc: 0.9750\n",
      "Epoch 20/20\n",
      "600/600 [==============================] - 0s 374us/step - loss: 0.0757 - acc: 0.9800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbbf5dbf470>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This is the traing stage that is compute intensive and may require high performance computers\n",
    "## or GPU machines if the size of the training dataset is huge\n",
    "## \n",
    "## In the begining of notebook, we decimated the size of the training dataset to finish our meeting on time\n",
    "## If you increase the epochs, the number of data repetition,\n",
    "## the running time will increase linearly proportional to the epochs.\n",
    "myclassifier.fit(Xtrain, Ytrain, epochs=20, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xpred = myclassifier.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.380131516456604, 0.9]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myclassifier.evaluate(Xtest, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the above, the test accuracy is 94%\n",
    "# Our model made wrong predictions in 6 cases out of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now, let's find out which image (handwriting) is not understood(classified) correctly.\n",
    "XpredFinal = [np.argmax(x) for x in Xpred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "XpredFinalArray = np.array(XpredFinal, dtype='float32')\n",
    "YtestFinalArray = Ytest.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Errors = Ytest - XpredFinalArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxErrorLocation = np.argmax(np.abs(Errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ximage = Xtest[maxErrorLocation,  :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ximage2d = ximage.reshape(ximage.shape[0], ximage.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbbf40e0ba8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(ximage2d)\n",
    "\n",
    "# this figure looks 4 but at the same time, it is close to 9.\n",
    "# our model predition is 9. But it is not totally wrong, I think.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# prediction \n",
    "print(XpredFinalArray[maxErrorLocation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# Correction Answer\n",
    "print(Ytest[maxErrorLocation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
